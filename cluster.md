# Working on the cluster

This file will give a brief guide on how to access the cluster of 
the computer science institute at the university of Potsdam and what to
keep in mind when using the benchmark tool on the cluster.

>[!NOTE]
>Keep in mind, this cluster is not the same as the university ZIM cluster
>accessible to all students. If you want to use this cluster instead,
>check the [ZIM HPC cluster][zim_hpc] page for more information.

## Getting access

To be able to access the cluster you need both an university and computer
science (CS) institute account. For information how to obtain said accounts check
the 'Account Setup' section in the [doc] repository.

After getting your accounts you have to add your ssh-key. This can be done using
the [account][cs_account] page. This page is only available in german.
Select 'SSH-Schlüssel bearbeiten' and log in with your CS credentials.
Aftwerwards paste your ssh-key into the big text field below 'Schlüssel hinzufügen',
enter your password and click 'Speichern'.

After getting your accounts, you have to add your SSH key. This can be done using
the [account][cs_account] page. This page is only available in German.
Select 'SSH-Schlüssel bearbeiten' and log in with your CS credentials.
Afterwards, paste your SSH key into the large text field below 'Schlüssel hinzufügen',
enter your password, and click 'Speichern'.

>[!NOTE]
>Changes to your CS account, e.g., setting a new password/ssh-keys may take
>some time to process.

Try accessing the cluster now by using the command below. The USERNAME is your
CS account name. You can disconnect from the cluster using `exit`.

```bash
ssh -J USERNAME@login.cs.uni-potsdam.de USERNAME@login.hpc.cs.uni-potsdam.de
```

To make it easier to connect you can additionally setup your `~/.ssh/config` file.
Add the lines below while replacing USERNAME with your CS account name and SSH-KEY
with name of your ssh-key.

```
Host cs-login
    User USERNAME
    Hostname login.cs.uni-potsdam.de
    IdentityFile ~/.ssh/SSH-KEY

Host hpc
    User USERNAME
    HostName login.hpc.cs.uni-potsdam.de
    IdentityFile ~/.ssh/SSH-KEY
    ProxyJump cs-login

Host *
    AddKeysToAgent yes
    ForwardAgent   yes
```

Afterwards you should be able to access the cluster using:

```bash
ssh hpc
```

For GUI applications you can enable X-forwarding by setting the `-X` option.
While this is enough for Linux, with MacOS and Windows you additionally need an
X-server.

More general information regarding the cluster can be found on its
[web page][cs_hpc]. This site is only available in german.

>[!NOTE]
>If you want to access the cluster from outside of the university network
>you need to use a VPN. An explanation on how to setup the VPN and connect
>to the university network can be found [here][vpn].

## Benchmark tool on the cluster

We recommend to install any python tools inside a conda environment, specifically
miniconda. miniconda installation instructions can be found [here][miniconda].

We recommend installing any Python tools inside a conda environment, specifically
Miniconda. Miniconda installation instructions can be found [here][miniconda].

After you have setup miniconda, you can install and use the benchmark tool as described
in the [getting started][get_started] section of the documentation. Keep in mind you should
never run sequential jobs on the cluster as these would run on the login node. Always use
distributed jobs.

You do not have to call the scheduler manually, simply execute the `start.sh` script
generated by `btool gen`. Since the the scheduler has a maximum number of jobs per user,
you will have to use `btool run-dist` if you have a large number of jobs.


### Systems under test using environments

If your system under test requires an environment to run, make sure to adjust the
`single.dist` template. To enable a conda environment, you can add the following lines,
where `<ENV>` is the name of your environment.

```bash
# Load environment modules for your application here.
source ~/.bashrc
source ~/miniconda3/bin/activate
source activate <ENV>
```

### Avoiding runlim errors

When running benchmarks on the cluster, jobs may fail due to the following error:

```bash
runlim error: group pid <X> larger than child pid <Y>
```

This is a known [issue][runlim_issue].

For single-process systems under test (SUT), this issue can be avoided by using the
`runlim` option `--single` in the corresponding template script
(e.g., `templates/seq-generic-single.sh`). In that case, `{run.solver}` should either be
the SUT executable or you should use `exec` if `{run.solver}` refers to a shell script.

If you cannot use `--single`, the verify subcommand can be used to identify jobs that
failed due to a `runlim error` and remove the corresponding `.finished` files. The gen
subcommand can then generate a new start script, which excludes valid jobs, by using
the `-e` option.

### Evaluating results

The file system on the cluster is rather slow. It is therefore recommended to pack all
results into an archive when all jobs are finished, copy the archive to your local machine,
and do any further steps, e.g., `btool eval`, afterwards. 

[zim_hpc]: https://www.uni-potsdam.de/en/zim/angebote-loesungen/hpc
[doc]: https://github.com/krr-up/doc
[cs_account]: https://account.cs.uni-potsdam.de/
[cs_hpc]: https://www.cs.uni-potsdam.de/bs/research/labs.html
[vpn]: https://www.uni-potsdam.de/en/zim/angebote-loesungen/vpn
[miniconda]: https://www.anaconda.com/docs/getting-started/miniconda/install#linux-2
[runlim_issue]: https://github.com/arminbiere/runlim/issues/8
